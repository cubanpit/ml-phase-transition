%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[a4paper, total={6in, 9in}]{geometry}

\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{amsmath} % Required for some math elements
\usepackage[italian]{babel}
\usepackage{booktabs}% Better table spacing
\usepackage{subfig}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
% axis style, ticks, etc
\pgfplotsset{every axis/.append style={
                    label style={font=\small},
                    tick label style={font=\footnotesize},
                    legend style={font=\scriptsize}
                    }}


\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

\usepackage{tikz}
\usetikzlibrary{positioning}
\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=1cm
  },
  neuron missing/.style={
    draw=none,
    scale=2,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}
\tikzstyle{line}=[draw]

% link and reference config
\usepackage{varioref}
\usepackage{hyperref}
\hypersetup{%
    pdfpagemode={UseOutlines},
    bookmarksopen,
    pdfstartview={FitH},
    colorlinks,
    linkcolor={black}, %COLORE DEI RIFERIMENTI AL TESTO
    citecolor={black}, %COLORE DEI RIFERIMENTI ALLE CITAZIONI
    urlcolor={black} %COLORI DEGLI URL
}

% compile tikz plots only once
\usetikzlibrary{trees}
\usetikzlibrary{external}
\usepgfplotslibrary{external}
\tikzexternalize[prefix=tex/]
\usepgfplotslibrary{units} % L A TEX and plain TEX
\usetikzlibrary{pgfplots.units} % L A TEX and plain TEX


%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Rete neuronale per Ising e XY} % Title

\author{Martina \textsc{Crippa}, Pietro Francesco \textsc{Fontana}} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date

%\begin{center}
%\begin{tabular}{l r}
%Date Performed: & January 1, 2012 \\ % Date the experiment was performed
%Partners: & James Smith \\ % Partner names
%& Mary Smith \\
%Instructor: & Professor Smith % Instructor/supervisor
%\end{tabular}
%\end{center}

% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduzione}
Nel campo della meccanica statistica un aspetto fondamentale è lo studio delle transizioni di fase, ovvero la trasformazione dello stato di un sistema al mutare di determinate variabili termodinamiche.
I sistemi considerati in questo lavoro, descritti dal modello di Ising e dal modello XY, transiscono al variare della temperatura e nel punto di passaggio questa viene detta temperatura di transizione.
In generale la transizione di fase è descritta da una funzione caratteristica del sistema, detta parametro d'ordine, ad esempio la magnetizzazione nel modello di Ising.
L'obiettivo del progetto è implementare e studiare una rete neuronale in grado di apprendere il parametro d'ordine per il modello di Ising e quindi classificare correttamente la fase di diverse configurazioni del sistema, individuando infine la temperatura di transizione.
Successivamente si è provato ad eseguire una procedura analoga nel caso del modello XY, un sistema che non presenta una transizione di fase descrivibile attraverso un parametro d'ordine, quindi dove la rete neuronale apprende altre caratteristiche del sistema per classificarne la fase.
Il lavoro prende spunto da diversi articoli pubblicati negli ultimi due anni che affrontano la stessa tematica \cite{carrasqu,melko,wessel}.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Sistemi fisici e simulazioni}
In questo lavoro sono stati simulati e studiati due sistemi classici della meccanica statistica su reticolo bidimensionale:  il modello di Ising e il modello XY.

\subsection{Modello di Ising}\label{sec:simIsing}
Il modello di Ising descrive il comportamento di spin su reticolo che assumono valori $\{+1;-1\}$ e interagiscono attraverso un'hamiltoniana la cui forma più semplice, utilizzata in questo lavoro, è
\begin{equation}
H=- \sum_{\langle i~j\rangle} \sigma_i\sigma_j
\end{equation}
dove le parentesi angolari indicano l'interazione a primi vicini fra gli spin $\sigma$.
Quindi viene assunto nullo il campo magnetico esterno e la costante di accoppiamento (solitamente indicata con $J$) assume lo stesso valore fra tutti gli spin, pari a $1$.
Ad alte temperature il contributo entropico fa sì che il sistema si trovi in uno stato di disordine, ovvero nella fase paramagnetica, illustrata in figura \ref{fig:isingP}.
Abbassando la temperatura prevale l'interazione tra gli spin e al di sotto della temperatura di transizione questi tendono ad allinearsi, quindi il sistema transisce alla fase ferromagnetica, rappresentata in figura \ref{fig:isingF}.
Tale transizione di fase è detta del \emph{secondo ordine} ed è caratterizzata da un comportamento critico, per questo la temperatura di transizione viene anche detta temperatura critica.

\begin{figure}[ht]
\centering
\subfloat[Paramagnetico]{
\centering
\begin{tikzpicture}[> = stealth, scale=0.8]
\draw[gray,dashed,step=1.5cm] (-1,-1) grid +(5cm,5cm);
    \foreach \y in {0,1,2}
        \foreach \x in {0,1,2}
            \node[shape = rectangle, minimum width = 0.75cm, minimum height = 0.75cm] (dot_\y_\x) at (1.5*\x,1.5*\y){};

   \draw[ultra thick, ->]  (dot_0_0.south) -- (dot_0_0.north);
   \draw[ultra thick, <-]  (dot_1_1.south) -- (dot_1_1.north);
   \draw[ultra thick, <-]  (dot_2_1.south) -- (dot_2_1.north);
   \draw[ultra thick, ->]  (dot_1_2.south) -- (dot_1_2.north);
   \draw[ultra thick, <-]  (dot_2_2.south) -- (dot_2_2.north);
   \draw[ultra thick, <-]  (dot_0_2.south) -- (dot_0_2.north);
   \draw[ultra thick, ->]  (dot_2_0.south) -- (dot_2_0.north);
   \draw[ultra thick, ->]  (dot_0_1.south) -- (dot_0_1.north);
   \draw[ultra thick, <-]  (dot_1_0.south) -- (dot_1_0.north);
\end{tikzpicture}
\label{fig:isingP}
}
\hspace{2cm}
\subfloat[Ferromagnetico]{
\centering
\begin{tikzpicture}[> = stealth, scale=0.8]
\draw[gray,dashed,step=1.5cm] (-1,-1) grid +(5cm,5cm);
    \foreach \y in {0,1,2}
        \foreach \x in {0,1,2}
            {\node[shape = rectangle, minimum width = 0.75cm, minimum height = 0.75cm] (dot_\y_\x) at (1.5*\x,1.5*\y){};
            \draw[ultra thick, ->]  (dot_\y_\x.south) -- (dot_\y_\x.north);}
\end{tikzpicture}
\label{fig:isingF}
}
\caption{Ising 2D}
\end{figure}

Il parametro d'ordine che evidenzia la transizione di fase è la magnetizzazione media per spin del sistema
\begin{equation}
\label{eq:magn}
m =\frac{1}{N} \sum_i \sigma_i
\end{equation}
che risulta vicina a 0 se valutata ad alte temperature, mentre a basse temperature tende a $+1$ o a $-1$.
Sono stati studiati sistemi con geometrie di reticolo differenti e conseguentemente aventi un numero di primi vicini (nn) e temperatura critica diversa, i valori sono riportati in tabella \ref{tab:ltI}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lllll}
\toprule
reticolo & nn & $T_c$ [$J/k_B$] & $T_c$ approx [$J/k_B$] & $T_{init}$ [$J/k_B$]\\
\midrule
honeycomb & $3$ & --- & $1.5187$ & $0.0$\\
quadrato & $4$ & $2/ \!\ln{(1+\sqrt{2})}$ & $2.2692$ & $1.0$\\
triangolare & $6$ & $4/\!\ln{3}$ & $3.6410 $ & $2.0$\\
\bottomrule
\end{tabular}
\end{center}
\caption{Temperatura critica per i reticoli studiati. Con $k_B$ si indica la costante di Boltzmann.}
\label{tab:ltI}
\end{table}

La simulazione dei vari sistemi è stata effettuata con metodi Monte Carlo per $40$ diverse temperature distribuite simmetricamente attorno alla temperatura critica, specificando la temperatura iniziale $T_{init}$ riportata in tabella \ref{tab:ltI}.
Il sistema è stato inizializzato a $T_{init}$, lasciandolo raggiungere l'equilibrio a temperatura fissata per poi passare alla temperatura successiva senza inizializzare nuovamente il sistema.
I modelli di Ising su reticolo quadrato e honeycomb sono stati simulati tramite l'algoritmo di Wolff \cite{wolff}, quindi eseguendo mosse a cluster, mentre il modello su reticolo triangolare è stato simulato con l'algoritmo di Metropolis \cite{metropolis}, quindi ad ogni mossa viene provata l'inversione di un singolo spin.
Nelle simulazioni con l'algoritmo di Wolff il sistema è inizializzato random, mentre nella simulazione con l'algoritmo di Metropolis il sistema è inizializzato ordinato per facilitare il raggiungimento dell'equilibrio a bassa temperatura.
In tutti i sistemi studiati vengono applicate condizioni al contorno periodiche.
Le simulazioni sono state scritte in linguaggio C++, il generatore di numeri casuali utilizzato è il Mersenne Twister 19937 \cite{mersenne}.

Nel caso del reticolo honeycomb le dimensioni si riferiscono ad un reticolo quadrato su cui si intrecciano due reticoli honeycomb, le interazioni sono progettate in modo che i due reticoli intrecciati si connettano lungo una direzione tramite condizioni al contorno periodiche, riproducendo il comportamento di un reticolo honeycomb unico con una dimensione doppia.
L'accuratezza del procedimento viene confermata dai risultati stessi del lavoro.

\subsection{Modello XY}

\begin{figure}[ht]
\centering
\subfloat[Modello XY vicino alla temperatura critica]{
\centering
\begin{tikzpicture}[> = stealth, scale=0.65]
\draw[gray,dashed,step=1.5cm] (-1,-1) grid +(9.5cm,9.5cm);
    \foreach \y in {0,1,2,3,4,5}
        \foreach \x in {0,1,2,3,4,5}
            \node[shape = circle, minimum size = 0.6cm] (dot_\y_\x) at (1.5*\x,1.5*\y){};

    \draw[ultra thick,black, ->] (dot_0_0.2.79 r) -- (dot_0_0.pi r + 2.79 r);
    \draw[ultra thick,black, ->] (dot_0_1.3.38 r) -- (dot_0_1.pi r + 3.38 r);
    \draw[ultra thick,black, ->] (dot_0_2.3.50 r) -- (dot_0_2.pi r + 3.50 r);
    \draw[ultra thick,black, ->] (dot_0_3.2.99 r) -- (dot_0_3.pi r + 2.99 r);
    \draw[ultra thick,black, ->] (dot_0_4.2.51 r) -- (dot_0_4.pi r + 2.51 r);
    \draw[ultra thick,black, ->] (dot_0_5.2.63 r) -- (dot_0_5.pi r + 2.63 r);
    \draw[ultra thick,black, ->] (dot_1_0.3.46 r) -- (dot_1_0.pi r + 3.46 r);
    \draw[ultra thick,black, ->] (dot_1_1.2.45 r) -- (dot_1_1.pi r + 2.45 r);
    \draw[ultra thick,black, ->] (dot_1_2.2.67 r) -- (dot_1_2.pi r + 2.67 r);
    \draw[ultra thick,black, ->] (dot_1_3.2.63 r) -- (dot_1_3.pi r + 2.63 r);
    \draw[ultra thick,black, ->] (dot_1_4.2.07 r) -- (dot_1_4.pi r + 2.07 r);
    \draw[ultra thick,black, ->] (dot_1_5.1.50 r) -- (dot_1_5.pi r + 1.50 r);
    \draw[ultra thick,black, ->] (dot_2_0.2.29 r) -- (dot_2_0.pi r + 2.29 r);
    \draw[ultra thick,black, ->] (dot_2_1.3.38 r) -- (dot_2_1.pi r + 3.38 r);
    \draw[ultra thick,black, ->] (dot_2_2.2.82 r) -- (dot_2_2.pi r + 2.82 r);
    \draw[ultra thick,black, ->] (dot_2_3.1.92 r) -- (dot_2_3.pi r + 1.92 r);
    \draw[ultra thick,black, ->] (dot_2_4.3.19 r) -- (dot_2_4.pi r + 3.19 r);
    \draw[ultra thick,black, ->] (dot_2_5.5.22 r) -- (dot_2_5.pi r + 5.22 r);
    \draw[ultra thick,black, ->] (dot_3_0.3.24 r) -- (dot_3_0.pi r + 3.24 r);
    \draw[ultra thick,black, ->] (dot_3_1.2.81 r) -- (dot_3_1.pi r + 2.81 r);
    \draw[ultra thick,black, ->] (dot_3_2.3.68 r) -- (dot_3_2.pi r + 3.68 r);
    \draw[ultra thick,black, ->] (dot_3_3.3.74 r) -- (dot_3_3.pi r + 3.74 r);
    \draw[ultra thick,black, ->] (dot_3_4.2.65 r) -- (dot_3_4.pi r + 2.65 r);
    \draw[ultra thick,black, ->] (dot_3_5.0.78 r) -- (dot_3_5.pi r + 0.78 r);
    \draw[ultra thick,black, ->] (dot_4_0.2.24 r) -- (dot_4_0.pi r + 2.24 r);
    \draw[ultra thick,black, ->] (dot_4_1.3.17 r) -- (dot_4_1.pi r + 3.17 r);
    \draw[ultra thick,black, ->] (dot_4_2.2.86 r) -- (dot_4_2.pi r + 2.86 r);
    \draw[ultra thick,black, ->] (dot_4_3.2.93 r) -- (dot_4_3.pi r + 2.93 r);
    \draw[ultra thick,black, ->] (dot_4_4.3.16 r) -- (dot_4_4.pi r + 3.16 r);
    \draw[ultra thick,black, ->] (dot_4_5.3.12 r) -- (dot_4_5.pi r + 3.12 r);
    \draw[ultra thick,black, ->] (dot_5_0.2.79 r) -- (dot_5_0.pi r + 2.79 r);
    \draw[ultra thick,black, ->] (dot_5_1.2.89 r) -- (dot_5_1.pi r + 2.89 r);
    \draw[ultra thick,black, ->] (dot_5_2.3.30 r) -- (dot_5_2.pi r + 3.30 r);
    \draw[ultra thick,black, ->] (dot_5_3.2.76 r) -- (dot_5_3.pi r + 2.76 r);
    \draw[ultra thick,black, ->] (dot_5_4.2.53 r) -- (dot_5_4.pi r + 2.53 r);
    \draw[ultra thick,black, ->] (dot_5_5.2.91 r) -- (dot_5_5.pi r + 2.91 r);

    \node[draw, blue, ultra thick, shape = circle, minimum size = 0.8cm] at (1.5*4.5,1.5*2.5){};
    \node[draw, red, ultra thick, shape = circle, minimum size = 0.8cm] at (1.5*4.5,1.5*1.5){};
\end{tikzpicture}
}
\hspace{2cm}
\subfloat[Modello XY sopra la temperatura critica]{
\centering
\begin{tikzpicture}[> = stealth, scale=0.65]
\draw[gray,dashed,step=1.5cm] (-1,-1) grid +(9.5cm,9.5cm);
    \foreach \y in {0,1,2,3,4,5}
        \foreach \x in {0,1,2,3,4,5}
            \node[shape = circle, minimum size = 0.6cm] (dot_\y_\x) at (1.5*\x,1.5*\y){};

    \draw[ultra thick,black, ->] (dot_0_0.5.05 r) -- (dot_0_0.pi r + 5.05 r);
    \draw[ultra thick,black, ->] (dot_0_1.4.26 r) -- (dot_0_1.pi r + 4.26 r);
    \draw[ultra thick,black, ->] (dot_0_2.4.36 r) -- (dot_0_2.pi r + 4.36 r);
    \draw[ultra thick,black, ->] (dot_0_3.3.22 r) -- (dot_0_3.pi r + 3.22 r);
    \draw[ultra thick,black, ->] (dot_0_4.2.15 r) -- (dot_0_4.pi r + 2.15 r);
    \draw[ultra thick,black, ->] (dot_0_5.2.22 r) -- (dot_0_5.pi r + 2.22 r);
    \draw[ultra thick,black, ->] (dot_1_0.5.04 r) -- (dot_1_0.pi r + 5.04 r);
    \draw[ultra thick,black, ->] (dot_1_1.5.15 r) -- (dot_1_1.pi r + 5.15 r);
    \draw[ultra thick,black, ->] (dot_1_2.5.38 r) -- (dot_1_2.pi r + 5.38 r);
    \draw[ultra thick,black, ->] (dot_1_3.1.31 r) -- (dot_1_3.pi r + 1.31 r);
    \draw[ultra thick,black, ->] (dot_1_4.3.55 r) -- (dot_1_4.pi r + 3.55 r);
    \draw[ultra thick,black, ->] (dot_1_5.1.98 r) -- (dot_1_5.pi r + 1.98 r);
    \draw[ultra thick,black, ->] (dot_2_0.4.08 r) -- (dot_2_0.pi r + 4.08 r);
    \draw[ultra thick,black, ->] (dot_2_1.5.87 r) -- (dot_2_1.pi r + 5.87 r);
    \draw[ultra thick,black, ->] (dot_2_2.0.30 r) -- (dot_2_2.pi r + 0.30 r);
    \draw[ultra thick,black, ->] (dot_2_3.4.95 r) -- (dot_2_3.pi r + 4.95 r);
    \draw[ultra thick,black, ->] (dot_2_4.5.95 r) -- (dot_2_4.pi r + 5.95 r);
    \draw[ultra thick,black, ->] (dot_2_5.1.60 r) -- (dot_2_5.pi r + 1.60 r);
    \draw[ultra thick,black, ->] (dot_3_0.2.28 r) -- (dot_3_0.pi r + 2.28 r);
    \draw[ultra thick,black, ->] (dot_3_1.0.76 r) -- (dot_3_1.pi r + 0.76 r);
    \draw[ultra thick,black, ->] (dot_3_2.5.55 r) -- (dot_3_2.pi r + 5.55 r);
    \draw[ultra thick,black, ->] (dot_3_3.5.54 r) -- (dot_3_3.pi r + 5.54 r);
    \draw[ultra thick,black, ->] (dot_3_4.5.77 r) -- (dot_3_4.pi r + 5.77 r);
    \draw[ultra thick,black, ->] (dot_3_5.5.86 r) -- (dot_3_5.pi r + 5.86 r);
    \draw[ultra thick,black, ->] (dot_4_0.1.68 r) -- (dot_4_0.pi r + 1.68 r);
    \draw[ultra thick,black, ->] (dot_4_1.0.32 r) -- (dot_4_1.pi r + 0.32 r);
    \draw[ultra thick,black, ->] (dot_4_2.4.10 r) -- (dot_4_2.pi r + 4.10 r);
    \draw[ultra thick,black, ->] (dot_4_3.5.35 r) -- (dot_4_3.pi r + 5.35 r);
    \draw[ultra thick,black, ->] (dot_4_4.0.97 r) -- (dot_4_4.pi r + 0.97 r);
    \draw[ultra thick,black, ->] (dot_4_5.1.40 r) -- (dot_4_5.pi r + 1.40 r);
    \draw[ultra thick,black, ->] (dot_5_0.6.19 r) -- (dot_5_0.pi r + 6.19 r);
    \draw[ultra thick,black, ->] (dot_5_1.0.50 r) -- (dot_5_1.pi r + 0.50 r);
    \draw[ultra thick,black, ->] (dot_5_2.1.56 r) -- (dot_5_2.pi r + 1.56 r);
    \draw[ultra thick,black, ->] (dot_5_3.0.56 r) -- (dot_5_3.pi r + 0.56 r);
    \draw[ultra thick,black, ->] (dot_5_4.0.81 r) -- (dot_5_4.pi r + 0.81 r);
    \draw[ultra thick,black, ->] (dot_5_5.1.47 r) -- (dot_5_5.pi r + 1.47 r);

    \node[draw, blue, ultra thick, shape = circle, minimum size = 0.8cm] at (1.5*2.5,1.5*4.5){};
    \node[draw, blue, ultra thick, shape = circle, minimum size = 0.8cm] at (1.5*0.5,1.5*2.5){};
    \node[draw, red, ultra thick, shape = circle, minimum size = 0.8cm] at (1.5*4.5,1.5*1.5){};
    \node[draw, red, ultra thick, shape = circle, minimum size = 0.8cm] at (1.5*2.5,1.5*0.5){};
\end{tikzpicture}
}
\caption{Modello XY a due diverse temperature, i vortici sono indicati in blu, gli antivortici in rosso. Si può notare una chiara coppia vortice-antivortice nella figura di sinistra, mentre nella figura di destra non è presente alcuna coppia.}
\label{fig:xy}
\end{figure}

Il modello XY descrive il comportamento di spin su reticolo in grado di ruotare assumendo valori nell'intervallo $[0,2\pi)$; gli spin interagiscono a primi vicini tramite l'hamiltoniana
\begin{equation}
H=-\sum_{\langle i~j\rangle} \cos(\theta_i-\theta_j)
\end{equation}
dove la costante si accoppiamento (solitamente indicata con $J$) è stata fissata pari ad $1$ ed è stato posto nullo il campo magnetico esterno.
Nel modello XY,  secondo il teorema di Mermin-Wagner \cite{mermin}, non è prevista alcuna transizione di fase del secondo ordine.
Tuttavia presenta una transizione di fase \emph{topologica}, ovvero la transizione BKT \cite{kosterlitz}, che coinvolge la creazione di vortici e antivortici, configurazioni topologicamente stabili.
Appena sotto la temperatura di transizione si osserva la formazione di coppie vortice-antivortice mentre non è energeticamente conveniente la creazione di vortici e antivortici singoli, abbassando la temperatura la maggior parte degli spin tende ad orientarsi in modo parallelo.
Al di sopra della temperatura critica vince il contributo entropico e compaiono vortici e antivortici non accoppiati, gli spin in generale non sono più allineati tra loro.
Due configurazioni esemplari a temperature diverse sono riportate in Figura \ref{fig:xy}.

Anche questo sistema è stato simulato tramite metodi Monte Carlo, utilizzando l'algoritmo di Wolff e seguendo una procedura analoga a quella utilizzata per il modello di Ising, i parametri sono riportati in tabella \ref{tab:ltXY}.
L'inizializzazione degli spin sul reticolo è casuale e non viene ripetuta per ogni passo di temperatura ma solo all'inizio della simulazione, a bassa temperatura.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lllll}
\toprule
reticolo & nn & $T_t$ & $T_t$ approx [$J/k_B$] & $T_{init}$ [$J/k_B$]\\
\midrule
quadrato & $4$ & --- & $0.893$ & $0.01$\\
\bottomrule
\end{tabular}
\end{center}
\caption{Temperatura critica per il reticolo studiato.}
\label{tab:ltXY}
\end{table}

\section{Visualizzazione dei dati}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    title=Honeycomb,
    colorbar horizontal,
    colorbar/width=0.2cm,
    colorbar shift/.style={yshift=0cm},
    colorbar style={xlabel=T [$J/k_B$]},
    xtick=\empty,
    ytick=\empty,
    colormap name=viridis,
    grid=major,
    width=0.31\textwidth,
    height=0.31\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot[scatter, only marks, mark=o, mark size=1pt, point meta=explicit]
    table[x=x, y=y, meta=temp] {dati/tsne/tsne_2500_hc};
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Quadrato,
    colorbar horizontal,
    colormap name=viridis,
    colorbar/width=0.2cm,
    colorbar shift/.style={yshift=0cm},
    colorbar style={xlabel=T [$J/k_B$]},
    xtick=\empty,
    ytick=\empty,
    grid=major,
    width=0.31\textwidth,
    height=0.31\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot[scatter, only marks, mark=o, mark size=1pt, point meta=explicit]
    table[x=x, y=y, meta=temp] {dati/tsne/tsne_2500_sq};
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Triangolare freddo,
    colorbar horizontal,
    colorbar/width=0.2cm,
    colorbar shift/.style={yshift=0cm},
    colorbar style={xlabel=T [$J/k_B$]},
    colormap name=viridis,
    xtick=\empty,
    ytick=\empty,
    grid=major,
    width=0.31\textwidth,
    height=0.31\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot[scatter, only marks, mark=o, mark size=1pt, point meta=explicit]
    table[x=x, y=y, meta=temp] {dati/tsne/tsne_2500_tr_cold};
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Triangolare caldo,
    colorbar horizontal,
    colorbar/width=0.2cm,
    colorbar shift/.style={yshift=0cm},
    colorbar style={xlabel=T [$J/k_B$]},
    colormap name=viridis,
    xtick=\empty,
    ytick=\empty,
    grid=major,
    width=0.31\textwidth,
    height=0.31\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot[scatter, only marks, mark=o, mark size=1pt, point meta=explicit]
    table[x=x, y=y, meta=temp] {dati/tsne/tsne_2500_tr_hot};
\end{axis}
\end{tikzpicture}
\caption{Riduzioni bidimensionali di \num{5000} configurazioni di modelli di Ising su reticoli di lato 50. Il colore indica la temperatura a cui la configurazione è stata simulata, la temperatura di transizione è sempre al centro della scala.}
\label{fig:tsne_ising}
\end{figure}

\begin{figure}
\centering
\subfloat{
\begin{tikzpicture}
\begin{axis}[
    title=XY Configurazioni,
    colorbar horizontal,
    colorbar/width=0.2cm,
    colorbar shift/.style={yshift=0cm},
    colorbar style={xlabel=T [$J/k_B$]},
    colormap name=viridis,
    xtick=\empty,
    ytick=\empty,
    grid=major,
    width=0.35\textwidth,
    height=0.35\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot[scatter, only marks, mark=o, mark size=1pt, point meta=explicit]
    table[x=x, y=y, meta=temp] {dati/tsne/tsne_1024_xy};
\end{axis}
\end{tikzpicture}
}
\hspace{2cm}
\subfloat{
\begin{tikzpicture}
\begin{axis}[
    title=XY Vortici,
    colorbar horizontal,
    colorbar/width=0.2cm,
    colorbar shift/.style={yshift=0cm},
    colorbar style={xlabel=T [$J/k_B$]},
    colormap name=viridis,
    xtick=\empty,
    ytick=\empty,
    grid=major,
    width=0.35\textwidth,
    height=0.35\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot[scatter, only marks, mark=o, mark size=1pt, point meta=explicit]
    table[x=x, y=y, meta=temp] {dati/tsne/tsne_1024_vortex};
\end{axis}
\end{tikzpicture}
}
\caption{Riduzione bidimensionale di \num{5000} configurazioni di un modello XY su reticolo di lato 32. Il colore indica la temperatura a cui la configurazione è stata simulata, la temperatura di transizione è sempre al centro della scala.}
\label{fig:tsne_xy}
\end{figure}

Prima di procedere con l'apprendimento è possibile visualizzare i dati tramite una riduzione bidimensionale delle configurazioni, alcuni esempi per il modello di Ising sono riportati in figura \ref{fig:tsne_ising}.
La tecnica utilizzata per ridurre la dimensionalità dei dati è t-SNE \cite{tsne}.
In tutte le figure è possibile osservare la creazione di cluster per i due possibili stati del sistema, al di sopra della temperatura di transizione e al di sotto di essa.

Sono stati generati due grafici per il reticolo triangolare; il primo basato sui dati di una simulazione in cui gli spin vengono inizializzati tutti a 1 quindi in uno stato naturale a bassa temperatura; il secondo proviene da una simulazione con inizializzazione casuale degli spin quindi in uno stato naturale ad alta temperatura.
La differenza tra le due figure di sistemi su reticolo triangolare è dovuta all'evoluzione del sistema a basse temperature, quando gli spin vengono inizializzati tutti a 1 è molto improbabile che la magnetizzazione si inverta completamente, mentre partendo da un'inizializzazione casuale il sistema può trovare l'equilibrio con eguale probabilità in una delle due polarizzazioni opposte.
Come indicato in sezione \ref{sec:simIsing} la simulazione su reticolo triangolare inizia da un sistema completamente ordinato per tutti i dati utilizzati nel resto del lavoro.

La riduzione di dimensionalità è stata applicata anche alle configurazioni di un modello XY su reticolo quadrato, il risultato è rappresentato in figura \ref{fig:tsne_xy}.
Fornendo le semplici configurazioni il risultato è poco chiaro, ma fornendo le configurazioni espresse in vortici (vd. sezione \ref{sec:xy_learning}) peggiora ulteriormente e compare un unico grosso cluster; in entrambi i casi molti punti fuoriescono dal cluster principale, mostrando come la classificazione non sia affatto completa.
Nel caso della figura a sinistra si possono notare due zone in cui sono concentrate le configurazioni a bassa temperatura, come nel caso di Ising, ma la simulazione non prevede magnetizzazione discreta nel caso di XY e l'inizializzazione del reticolo avviene in modo casuale, le configurazioni a bassa temperatura appaiono quindi come spin equi-orientati ma in direzione diversa per ogni singola simulazione.

In conclusione questa tecnica può fornire un'utile anteprima della struttura dei dati nel caso dei sistemi di Ising, al contrario nel caso dei sistemi XY non permette di trarre conclusioni, confermando la difficoltà presenti nella classificazione delle configurazioni di questo modello.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Metodi di apprendimento}
A partire dai sistemi fisici simulati, descritti nella sezione precedente, sono stati sviluppati diversi metodi di apprendimento con l'obiettivo di classificare automaticamente la fase in cui si trova il sistema.
I metodi utilizzati per l'apprendimento differiscono per i due modelli fisici analizzati, il modello di Ising e il modello XY, seguendo la stessa strada di altri lavori \cite{wessel}.

Le reti sono state implementate tramite Keras \cite{keras}, utilizzando Tensorflow \cite{tensorflow} come backend.

\subsection{Modello di Ising}

\begin{figure}[ht]
\centering
\resizebox{0.65\textwidth}{!}{
\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth, scale=1]

\foreach \m/\l [count=\y] in {1,2,missing,3}
  \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y*1.35) {};

\foreach \m [count=\y] in {1,2,missing,3}
  \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2.5-\y*1.35) {};

\node [circle,fill,inner sep=0.75pt] at (2.9166,2.5-1*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (3.25,2.5-1*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (3.5833,2.5-1*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (2.9166,2.5-2*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (3.25,2.5-2*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (3.5833,2.5-2*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (2.9166,2.5-4*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (3.25,2.5-4*1.35) {};
\node [circle,fill,inner sep=0.75pt] at (3.5833,2.5-4*1.35) {};
\node [every neuron/.try, neuron missing/.try ] (hidden-missing) at (3.25,2.5-3*1.35) {};

\foreach \m [count=\y] in {1,2,missing,3}
  \node [every neuron/.try, neuron \m/.try ] (shidden-\m) at (4.5,2.5-\y*1.35) {};

\foreach \m [count=\y] in {1,2}
  \node [every neuron/.try, neuron \m/.try ] (output-\m) at (6.5,1-\y) {};

\foreach \l [count=\i] in {1,2,n}
  \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$I_\l$};

\foreach \l [count=\i] in {1,2,m}
  \node [above] at (hidden-\i.north) {$H^1_\l$};

\foreach \l [count=\i] in {1,2,k}
  \node [above] at (shidden-\i.north) {$H^N_\l$};

\foreach \l [count=\i] in {1,2}
  \draw [->] (output-\i) -- ++(1,0)
    node [above, midway] {$O_\l$};

\foreach \i in {1,...,3}
  \foreach \j in {1,...,3}
    \draw [->] (input-\i) -- (hidden-\j);

\foreach \i in {1,...,3}
  \foreach \j in {1,...,2}
    \draw [->] (shidden-\i) -- (output-\j);

\node [align=center, above] at (0,2) {Layer \\ di Input};
\node [align=center, above] at (2,2) {Primo Layer \\ Nascosto};
\node [align=center, above] at (4.5,2) {N-esimo Layer \\ Nascosto};
\node [align=center, above] at (6.5,2) {Layer \\ di Output};

\end{tikzpicture}
}
\caption{Schema di una rete neuronale feed forward a più layer nascosti fully connected.}
\label{fig:ffn}
\end{figure}

Per classificare le configurazioni di un modello di Ising è stata utilizzata una rete neuronale \emph{feed forward} a uno o più layer nascosti \emph{fully connected}, uno schema generico della rete è rappresentato in Figura \ref{fig:ffn}.
Al layer di input vengono fornite le configurazioni degli spin del sistema fisico nella forma di un vettore di elementi $\{+1;-1\}$, la dimensione di questo vettore dipende ovviamente dalla dimensione del reticolo di Ising utilizzato, nel caso del presente lavoro spazia da un minimo di 400 elementi (reticolo $20\times20$) ad un massimo di 8100 elementi (reticolo $90\times90$).
Le etichette con cui vengono classificate le configurazioni, e che vengono fornite per l'apprendimento, sono variabili binarie che indicano se il sistema si trova sopra o sotto la temperatura di transizione.
In particolare vengono fornite coppie di valori ([1,0] o [0,1]) come etichette, per ottenere due valori in output.
Essendo un problema di classificazione binaria è possibile utilizzare uno o due nodi nel layer di output, per una questione di comodità nell'analisi dei risultati fisici è stato utilizzato un layer di output a due nodi.
I nodi di output restituiscono la probabilità che la configurazione fisica sia in una fase ferromagnetica o in quella paramagnetica, le due probabilità sommano quindi a 1.

La rete viene sempre allenata su configurazioni del modello di Ising su reticolo quadrato classico, ossia con 4 primi vicini; se riesce ad apprendere correttamente la transizione di fase dovrebbe essere in grado di classificare adeguatamente anche le configurazioni dei modelli a lei sconosciuti, come Ising su reticolo triangolare e honeycomb.

\subsection{Modello XY}
\label{sec:xy_learning}

\begin{figure}[ht]
 \centerline{\includegraphics[scale=0.35]{cnn.png}}
  \caption{Schema di una rete convoluzionale effettivamente utilizzata per l'analisi di un sistema XY su reticolo $32\times32$, sono stati tralasciati i layer \emph{dropout}. Immagine generata adattando il codice disponibile presso \url{https://github.com/gwding/draw_convnet}.}
 \label{fig:cnn}
\end{figure}

Il metodo migliore per classificare un modello XY risulta essere una rete convoluzionale \cite{melko}, uno schema di una rete di questo tipo effettivamente utilizzata in questo lavoro è rappresentato in Figura \ref{fig:cnn}.
I primi due layer, detti convoluzionali, sono fondamentali per estrarre le caratteristiche (\emph{feature}) che permettono la classificazione delle configurazioni.
La struttura della rete è quella tipica utilizzata per la classificazione di immagini, per esempio il riconoscimento di caratteri scritti a mano del database MNIST.

Il layer di input può ricevere come argomento la semplice configurazione degli spin, quindi una matrice di valori reali nell'intervallo $[0,2\pi)$, oppure la configurazione espressa in vortici e antivortici, che appare come una matrice con valori $\{-1;0;+1\}$ che rappresentano rispettivamente la presenza di un antivortice, l'assenza di strutture e la presenza di un vortice.
Come nel caso di Ising le etichette indicano se la temperatura del sistema si trova sopra o sotto la temperatura di transizone e il layer di output ha due nodi, che rappresentano le probabilità complementari di essere in una delle due fasi.


%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{Impostazione dell'apprendimento}

\begin{figure}[!ht]
\centering
\subfloat[Senza regolarizzatori]{
\begin{tikzpicture}
\begin{axis}[
    xlabel = {Epoche},
    ylabel = {Loss},
    grid = major,
    width=0.33\textwidth,
    height=0.4\textwidth,
    ymin=0,
    ymax=0.4,
    xmin=0,
    xmax=100,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[mark=none, smooth, dotted, very thick]
    table[x=epoch, y=loss] {dati/training_history/training_histo_xy_overfit};
\addlegendentry{Training loss}
\addplot+[mark=none, smooth, very thick]
    table[x=epoch, y=val_loss] {dati/training_history/training_histo_xy_overfit};
\addlegendentry{Validation loss}
\end{axis}
\end{tikzpicture}
}
\hfill
\subfloat[Con regolarizzatori]{
\begin{tikzpicture}
\begin{axis}[
    xlabel = {Epoche},
    grid = major,
    width=0.33\textwidth,
    height=0.4\textwidth,
    ymin=0,
    ymax=0.4,
    xmin=0,
    xmax=100,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[mark=none, smooth, dotted, very thick]
    table[x=epoch, y=loss] {dati/training_history/training_histo_xy_reg};
\addlegendentry{Training loss}
\addplot+[mark=none, smooth, very thick]
    table[x=epoch, y=val_loss] {dati/training_history/training_histo_xy_reg};
\addlegendentry{Validation loss}
\end{axis}
\end{tikzpicture}
}
\hfill
\subfloat[Early stop]{
\begin{tikzpicture}
\begin{axis}[
    xlabel = {Epoche},
    grid = major,
    width=0.33\textwidth,
    height=0.4\textwidth,
    ymin=0,
    ymax=0.4,
    xmin=0,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[mark=none, smooth, dotted, very thick]
    table[x=epoch, y=loss] {dati/training_history/training_histo_xy_stop};
\addlegendentry{Training loss}
\addplot+[mark=none, smooth, very thick]
    table[x=epoch, y=val_loss] {dati/training_history/training_histo_xy_stop};
\addlegendentry{Validation loss}
\end{axis}
\end{tikzpicture}
}
\caption{Risultati dell'apprendimento di una rete convoluzionale, del tipo rappresentato in figura \ref{fig:cnn}. I dati utilizzati sono relativi ad un modello XY su reticolo quadrato di lato 32.}
\label{fig:history}
\end{figure}

Il primo passo nel lavoro di creazione della rete è stato valutare se questa potesse apprendere efficacemente da un insieme di dati sufficientemente vasto.
Dopo aver generato circa \num{100000} configurazioni per l'apprendimento queste sono state divise in \emph{training set} e \emph{validation set} in una percentuale rispettivamente dell'$80\%$ e del $20\%$.
Attraverso il validation set è possibile stimare le performance della rete con l'avanzare del processo di apprendimento e capire se sta avvenendo un \emph{overfit}.

In figura \ref{fig:history} è possibile confrontare tre processi di apprendimento della rete convoluzionale utilizzata per la classificazione dei sistemi XY, vengono rappresentate la \emph{training loss} e la \emph{validation loss}, quando la seconda inizia a crescere si sta verificando un overfit, la rete sta imparando caratteristiche specifiche del training set e non generalizzabili ai dati sconosciuti del validation set.
Nel caso della figura a sinistra non è stato utilizzato alcuno strumento di regolarizzazione; si vede immediatamente come la rete inizia un overfit già dopo la decima epoca.
Per la figura al centro sono stati aggiunti due layer \emph{dropout}, uno prima ed uno dopo il layer nascosto da 128 neuroni, inoltre viene imposto un valore massimo pari a 5 alla norma dei pesi di questo layer nascosto; in questo caso la rete ha maggiori difficoltà a specializzarsi sui dati del training set.
Infine nella figura di destra viene applicato un \emph{early stop}, ossia l'apprendimento viene bloccato in automatico quando una certa quantità (in questo caso la \emph{validation accuracy}) non cresce significativamente per diverse epoche; si osserva nell'immagine che il processo viene efficacemente interrotto quando la validation loss non ha ancora iniziato a crescere.
%TODO: controllare struttura rete, quanti layer nascosti? flatten rientra tra i layer? è corretto?

Nell'apprendimento di sistemi di Ising il fenomeno di overfit è più contenuto, ma ai layer nascosti è stato ugualmente aggiunto un regolarizzatore L2, che limità la libertà di evoluzione dei pesi della rete impedendo la specializzazione sul training set.

\subsection{Statistica sui dati} \label{sec:stats}
Per migliorare l'affidabilità delle variabili misurate, come la test accuracy, sono stati generati test set da \num{100000} elementi che vengono divisi equamente in 10 insiemi su cui ogni modello viene valutato.
Allo stesso modo, per tenere conto della componente stocastica presente nel processo di apprendimento di una rete, sono stati generati 10 modelli per ciascuna rete sullo stesso training set.
Questo procedimento genera quindi 10 misure per ciascuno dei 10 modelli, per un totale di 100 misure, da cui viene estratta la media e l'errore, come deviazione standard della media.
Questo sarà lo standard utilizzato in tutte le analisi successive.

Si osserva che le misure effettuate non sono indipendenti tra loro, l'intero insieme di 100 misure condivide il medesimo training set e a gruppi di 10 condividono lo stesso modello, quindi non è possibile applicare né i teoremi del limite centrale né la legge dei grandi numeri.
Le medie e le deviazioni standard delle medie vengono ugualmente utilizzate nel lavoro perché sono lo strumento più semplice a disposizione.
Si potrebbero ottenere misure indipendenti partendo da un training set diverso per ogni misura, ma questa procedura richiede un impegno computazionale non praticabile.

%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Modello di Ising: risultati e conclusioni}

\subsection{Valutazione della geometria della rete}
Per meglio valutare il contributo dei diversi parametri della rete sono stati generati più modelli, eseguendo la fase di apprendimento sullo stesso training set e variando di volta in volta alcune caratteristiche quali il numero di neuroni in un layer o il numero di layer.
In questa sezione le principali caratteristiche  della rete che non vengono variate sono riportati in tabella \ref{tab:ffnnfixed}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{ll}
\toprule
Batch size: & 100\\
Pesi w e b iniziali: & Gauss($\nu=0$, $\sigma=1$)\\
Fz. attivazione: & Sigmoid\\
Earlystop: & validation accuracy \\
Regolarizzazione: & L2 0.01\\
Ottimizzazione: & Adam 0.0001 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Caratteristiche della rete fissate.}
\label{tab:ffnnfixed}
\end{table}

\subsubsection{Diverso numero di neuroni}
\label{sec:NN}
La prima analisi è stata effettuata allenando diverse reti a singolo layer nascosto con numero di neuroni variabile, i valori esplorati sono: 3, 10, 50, 100, 200 e 1000.
L'analisi è stata limitata a modelli di Ising su reticoli di lato 30, 50 e 80.
Nel grafico in figura \ref{fig:NN} sono riportati i valori di test accuracy al variare del numero di neuroni, rappresentati in scala logaritmica.

\begin{figure}[ht]
\centering
\subfloat{
\begin{tikzpicture}
\begin{axis}[
    title=Honeycomb,
    xmode=log,
    xlabel = {Numero di neuroni},
    ylabel = {Test accuracy},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_900_hc};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_2500_hc};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_6400_hc};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
}
\hfill
\subfloat{
\begin{tikzpicture}
\begin{axis}[
    title=Quadrato,
    xmode=log,
    xlabel = {Numero di neuroni},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_900_sq};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_2500_sq};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_6400_sq};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
}
\hfill
\subfloat{
\begin{tikzpicture}
\begin{axis}[
    title=Triangolare,
    xmode=log,
    xlabel = {Numero di neuroni},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_900_tr};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_2500_tr};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons, y=accuracy, y error=stdev] {dati/neurons_number/neurons_number_6400_tr};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
}
\caption{Performance della rete feed forward a singolo layer fully connected sul modello di Ising.}
\label{fig:NN}
\end{figure}

Si osserva che per il reticolo di lato 30 una rete a 10 neuroni è sufficiente per classificare in maniera adeguata la fase del sistema, raggiungendo una test accuracy circa del $98\%$.
Per reticoli di maggiori dimensioni sono necessari almeno 50 neuroni per raggiungre livelli di test accuracy superiori al $90\%$.
Per tutte le dimensioni considerate aumentando il numero di neuroni a 1000 si osserva un crollo nelle performance per i reticoli honeycomb e quadrato, mentre per il reticolo triangolare la rete continua a classificare in modo soddisfacente.
Per limiti computazionali non sono state realizzate reti con un numero di neuroni superiore, quindi non è stato possibile chiarire l'andamento complessivo per il reticolo triangolare.
Si osserva che una differenza tra la configurazione su reticolo triangolare e gli altri due sistemi, oltre alla geometria del reticolo, è l'algoritmo utilizzato per la simulazione del sistema, come riportato in sezione \ref{sec:simIsing}.
%TODO approfondire la cosa


\subsubsection{Diverso numero di layer}
\label{sec:NL}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel = {Numero di neuroni},
    ylabel = {\# parametri / \# training data},
    grid = major,
    width=0.4\textwidth,
    height=0.4\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north west,
]
\addplot+[line, thick, mark=*]
    table[x=neurons, y expr={\thisrow{param}/\thisrow{datatr}}] {dati/neurons_number/neurons_param_900};
\addlegendentry{L30}
\addplot+[line, thick, mark=square*]
    table[x=neurons, y expr={\thisrow{param}/\thisrow{datatr}}] {dati/neurons_number/neurons_param_2500};
\addlegendentry{L50}
\addplot+[line, thick, mark=triangle*]
    table[x=neurons, y expr={\thisrow{param}/\thisrow{datatr}}] {dati/neurons_number/neurons_param_6400};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\hspace{2cm}
\begin{tikzpicture}
\begin{axis}[
    xlabel = {Numero di layer nascosti},
    ymax=1.5,
    grid = major,
    width=0.4\textwidth,
    height=0.4\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north west,
]
\addplot+[line, thick, mark=*]
    table[x=layers, y expr={\thisrow{param}/\thisrow{datatr}}] {dati/layers_number/layers_param_900};
\addlegendentry{L30}
\addplot+[line, thick, mark=square*]
    table[x=layers, y expr={\thisrow{param}/\thisrow{datatr}}] {dati/layers_number/layers_param_2500};
\addlegendentry{L50}
\addplot+[line, thick, mark=triangle*]
    table[x=layers, y expr={\thisrow{param}/\thisrow{datatr}}] {dati/layers_number/layers_param_6400};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\caption{Rapporto tra il numero di parametri della rete e il numero di elementi del training set utilizzato per l'apprendimento.}
\label{fig:par}
\end{figure}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    title=Honeycomb,
    xlabel = {Numero di layer nascosti},
    ylabel = {Test accuracy},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_900_hc};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_2500_hc};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_6400_hc};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Quadrato,
    xlabel = {Numero di layer nascosti},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_900_sq};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_2500_sq};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_6400_sq};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Triangolare,
    xlabel = {Numero di layer nascosti},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_900_tr};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_2500_tr};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=layers, y=accuracy, y error=stdev] {dati/layers_number/layers_number_6400_tr};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\caption{Performance della rete feed forward a layer multipli fully connected da dieci neuroni ciascuno sul modello di Ising.}
\label{fig:LN}
\end{figure}

Il primo grafico riportato in figura \ref{fig:par} presenta l'andamento del numero di parametri normalizzato sul numero di training data rispetto al numero di neuroni per reti a un layer nascosto.
Come si può notare il numero di parametri segue un andamento lineare e cresce velocemente.
Diversamente, nel secondo grafico in figura \ref{fig:par} si osserva che aumentando il numero di layer il numero di parametri cresce sempre linearmente ma in maniera molto meno marcata, la crescita è rappresentata rispetto al numero di layer nascosti che è proporzionale al numero totale di neuroni.
Le specifiche delle reti a più layer sono specificate successivamente.

Nell'ambito delle reti neuronali si cerca di avere sempre un equilibrio tra il numero di parametri da apprendere ed il numero di dati utilizzati per l'apprendimento, preferendo se possibile una prevalenza della seconda quantità.
Si può trovare un parallelo con il numero di vincoli necessari per ottenere una soluzione unica in un sistema a molte variabili.
Un metodo per diminuire il numero di parametri totali è aumentare il numero di layer nascosti diminuendo i neuroni presenti su ciascun layer.

È stata effettuata quindi un'analisi allenando reti a più layer nascosti, pari a  1, 2, 3, 5 e 10, con ciascun layer popolato da 10 neuroni.
I risultati, riportati nei grafici di \ref{fig:LN}, mostrano che si ottiene un valore ottimale di test accuracy per tutte le tipologie di reticolo classificando con reti a 2 layer. Aggiungendo un layer le performance si abbassano notevolmente, mentre per i reticoli di lato 30 si ottengono buoni risultati anche per reti aventi  un singolo layer.
Inoltre dai risultati di questa analisi si può osservare come le performance sul modello di Ising con reticolo triangolare siano consistenti con quelle degli altri modelli, al contrario di quanto riportato nella sezione precedente.
%TODO: da approfondire

In definitiva, confrontando i valori in tabella \ref{tab:NNLN}, si può notare come l'apprendimento sul modello di Ising non sembri trarre vantaggio dall'utilizzo di reti a più layer.
Nel presente lavoro la sovrabbondanza di parametri nella rete rispetto al numero di dati non sembra penalizzare l'apprendimento della rete stessa.

\begin{table}[ht]
\begin{center}
\begin{tabular}{llll}
\toprule
Rete & L30 TA & L50 TA & L80 TA \\
\midrule
1x100 & \num{0.981 \pm 0.004} & \num{0.973 \pm 0.006} & \num{0.994 \pm 0.002} \\
2x10 & \num{0.979 \pm 0.004} & \num{0.988 \pm 0.003} & \num{0.993 \pm 0.002} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Test accuracy valutata su una rete a singolo layer da 100 neuroni e su una rete a 2 layer da 10 neuroni ciascuno. I valori sono la media sulle tre diverse tipologie di reticolo per ciascuna dimensione, l'errore è stimato come deviazione standard della media.}
\label{tab:NNLN}
\end{table}

\subsubsection{Due layer e diverso numero di neuroni}\label{sec:2Ln}
I risultati delle analisi descritte nei capitoli precedenti, riportati in tabella \ref{tab:NNLN}, mostrano che per tutti modelli considerati si ottengono valori di test accuracy massimi per reti a singolo layer con 100 neuroni e per reti a 2 layer con 10 neuroni ciascuno.

Prendendo spunto da questi risultati sono stati generati modelli di rete a 2 layer con un numero totale di neuroni pari a 100, variando il numero di neuroni dei singoli layer.
È stata allenata una rete a 30 e 70 neuroni, quindi fornendo ad entrambi i layer un numero consistente di neuroni.
Come si può osservare dai risultati riportati in tabella \ref{tab:2LN30-70}, questi sono compatibili entro l'errore con quelli in tabella \ref{tab:NNLN}, a parte la test accuracy per la rete 1x100 sul reticolo L50 che si discosta di circa $2\sigma$ dagli altri valori.
Anche invertendo i layer da 30 e 70 neuroni si ottengono valori di test accuracy equivalenti, considerando l'errore.

\begin{table}[ht]
\begin{center}
\begin{tabular}{llll}
\toprule
Rete & L30 TA & L50 TA & L80 TA\\
\midrule
1x70+1x30 & \num{0.979 \pm 0.004} & \num{0.988 \pm 0.003} & \num{0.993 \pm 0.002} \\
1x30+1x70 & \num{0.978 \pm 0.005} & \num{0.988 \pm 0.003} & \num{0.993 \pm 0.002} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Test accuracy (TA) valutata su una rete a 2 layer da 70 e 30 neuroni e su una rete a 2 layer da 30 e 70 neuroni. I valori sono la media sulle tre diverse tipologie di reticolo per ciascuna dimensione, l'errore è stimato come deviazione standard della media.}
\label{tab:2LN30-70}
\end{table}

Quindi, nel caso di una rete a due layer entrambi con un numero consistente di neuroni, l'inversione dei layer non sembra aver modificato le sue capacità di apprendimento.
Si prosegue allenando una rete con un numero di neuroni per layer meno equilibrato: 95 e 5.

\begin{table}[ht]
\begin{center}
\begin{tabular}{llll}
\toprule
Rete & L30 TA & L50 TA & L80 TA \\
\midrule
1x95+1x5 & \num{0.979 \pm 0.004} & \num{0.988 \pm 0.003} & \num{0.994 \pm 0.002} \\
1x5+1x95 & \num{0.978 \pm 0.004} & \num{0.987 \pm 0.003} & \num{0.992 \pm 0.003} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Test accuracy (TA) valutata su una rete a 2 layer da 95 e 5 neuroni e su una rete a 2 layer da 5 e 95 neuroni. I valori sono la media sulle tre diverse tipologie di reticolo per ciascuna dimensione, l'errore è stimato come deviazione standard della media.}
\label{tab:2LN5-95}
\end{table}

Anche in questo caso i risultati riportati in tabella \ref{tab:2LN5-95} sono assolutamente compatibili con i risultati in tabella \ref{tab:2LN30-70}, quindi anche una rete a due layer con un numero di neuroni sbilanciato non risulta essere sensibile all'inversione di layer.

Provando a sbilanciare ulteriormente il numero di neuroni dei due layer, ovvero allenando una rete con un layer da 98 neuroni e un layer da 2 neuroni, si è ottenuto un risultato interessante: la rete con il primo layer da 98 neuroni e il secondo da 2 classifica correttamente la fase del sistema con una test accuracy alta, mentre invertendo i layer si ottengono risultati di test accuracy molto bassi.
Questo è un segnale del fatto che 2 neuroni non sempre sono sufficienti per passare le informazioni necessarie per la classificazione al layer successivo, viene persa parte dell'informazione.
Si è osservato che 5 neuroni nel primo layer sono sufficienti per classificare correttamente, per trovare il numero minimo di neuroni per una buona classificazione si è provato ad allenare reti 97-3, 96-4 e le relative reti inverse ottenendo i risultati riportati in tabella \ref{tab:2LN2-3-4}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{llll}
\toprule
Rete & L30 TA & L50 TA & L80 TA \\
\midrule
1x98+1x2 & \num{0.977 \pm 0.005} & \num{0.940 \pm 0.003} & \num{0.993 \pm 0.002}\\
1x2+1x98 & \num{0.72 \pm 0.02} & \num{0.67\pm 0.02} & \num{0.68 \pm 0.02}\\
\midrule
1x97+1x3 & \num{0.979 \pm 0.004} & \num{0.989 \pm 0.003} & \num{0.994 \pm 0.003}\\
1x3+1x97 & \num{0.852 \pm 0.006} & \num{0.920 \pm 0.004} & \num{0.906 \pm 0.001}\\
\midrule
1x96+1x4 & \num{0.979 \pm 0.004} & \num{0.989 \pm 0.003} & \num{0.994 \pm 0.002}\\
1x4+1x96 & \num{0.946 \pm 0.004} & \num{0.970 \pm 0.003} & \num{0.992 \pm 0.003}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Test accuracy (TA) per diverse reti. I valori sono le medie e le deviazioni standard dei risultati ottenuti per il reticolo honeycomb, quadrato e triangolare.}
\label{tab:2LN2-3-4}
\end{table}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    title=Honeycomb,
    xlabel = {Numero di neuroni 1\textsuperscript{o} layer},
    ylabel = {Test accuracy},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    ymin=0.6,
    ymax=1.02,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_900_hc};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_2500_hc};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_6400_hc};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Quadrato,
    xlabel = {Numero di neuroni 1\textsuperscript{o} layer},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    ymin=0.6,
    ymax=1.02,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_900_sq};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_2500_sq};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_6400_sq};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Triangolare,
    xlabel = {Numero di neuroni 1\textsuperscript{o} layer},
    grid = major,
    width=0.35\textwidth,
    height=0.40\textwidth,
    ymin=0.6,
    ymax=1.02,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south east,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_900_tr};
\addlegendentry{L30}
\addplot+[line, mark=square*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_2500_tr};
\addlegendentry{L50}
\addplot+[line, mark=triangle*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=neurons1, y=accuracy, y error=stdev] {dati/layers_neuron_number/layers_neuron_number_l1low_6400_tr};
\addlegendentry{L80}
\end{axis}
\end{tikzpicture}
\caption{Performance della rete a 2 layer nascosti con numero di neuroni totali pari a 100.}
\label{fig:2L1Lln}
\end{figure}

In figura \ref{fig:2L1Lln} sono riportati i grafici relativi a reti a due layer con il primo layer di piccole dimensioni.
Si osserva che, diversamente dalle reti a singolo layer trattate in sezione \ref{sec:NN}, sono sufficienti 3 neuroni per ottenere una test accuracy superiore al $90\%$ con reticoli di lato 50 e 80, mentre per i reticoli di lato 30 si raggiunge una test accuracy (in media sulla tipologia di reticolo, vd tabella \ref{tab:2LN2-3-4}) superiore all'$85\%$.
Questo sottolinea come il secondo layer abbia un ruolo non indifferente nella capacità di apprendimento della rete.

\subsection{Valutazione dei parametri della rete}
Dopo aver esplorato la geometria della rete, si è provato ad allenare reti variando altri tre parametri: la funzione di attivazione, l'ottimizzatore e la batch size.

\subsubsection{Funzione di attivazione: Sigmoid vs ReLu vs Tanh}

\begin{figure}[ht]
\centering
\subfloat[Sigmoid]{
\begin{tikzpicture}
\begin{axis}[
    xmin=-3, xmax=3,
    ymin=-1.2, ymax=1.2,
    width=0.4\textwidth,
    height=0.4\textwidth,
    axis lines=center,
    axis on top=true,
    domain=-3:3,
    ylabel=$y$,
    xlabel=$x$,
    ]
    \addplot [mark=none, thick, smooth] {1/(1+exp(-\x))};
    %% Add the asymptotes
    \draw [dotted, thick] (axis cs:+10,+1)-- (axis cs:0,+1);
\end{axis}
\end{tikzpicture}
}
\hfill
\subfloat[Tanh]{
\begin{tikzpicture}
\begin{axis}[
    xmin=-3, xmax=3,
    ymin=-1.2, ymax=1.2,
    width=0.4\textwidth,
    height=0.4\textwidth,
    axis lines=center,
    axis on top=true,
    domain=-3:3,
    ylabel=$y$,
    xlabel=$x$,
    ]
    \addplot [mark=none, thick, smooth] {tanh(\x)};
    %% Add the asymptotes
    \draw [dotted, thick] (axis cs:-10,-1)-- (axis cs:0,-1);
    \draw [dotted, thick] (axis cs:+10,+1)-- (axis cs:0,+1);
\end{axis}
\end{tikzpicture}
}
\hfill
\subfloat[ReLu]{
\begin{tikzpicture}
\begin{axis}[
    xmin=-3, xmax=3,
    ymin=-1.2, ymax=1.2,
    width=0.4\textwidth,
    height=0.4\textwidth,
    axis lines=center,
    axis on top=true,
    domain=-3:3,
    ylabel=$y$,
    xlabel=$x$,
    ]
    \addplot [mark=none, thick] {max(0,\x)};
\end{axis}
\end{tikzpicture}
}
\caption{Funzioni di attivazione studiate.}
\label{fig:actv_func}
\end{figure}

Le tre funzioni riportate in figura \ref{fig:actv_func} sono le più comuni funzioni di attivazione utilizzate nelle reti neuronali: le reti analizzate in precedenza sono state implementate con la funzione di attivazione Sigmoid, utilizzando la medesima funzione di attivazione per tutti i layer nascosti.

In questa sezione sono state testate reti a singolo layer con 100 neuroni utilizzando queste tre funzioni di attivazione.
Si riportano i risultati per la rete 1x100 in tabella \ref{tab:SigVsReluVsTanhS}, mediando sulle tipologie di reticolo.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{llll}
\toprule
Rete & L30 TA & L50 TA & L80 TA \\
\midrule
Sigmoid & \num{0.981\pm 0.004} & \num{0.973\pm 0.006} & \num{0.994\pm 0.002}\\
Tanh & \num{0.981\pm 0.004} & \num{0.990 \pm 0.003} &\num{ 0.994\pm 0.002}\\
ReLu  & \num{0.833\pm 0.004} & \num{0.91 \pm 0.01} &\num{ 0.84\pm 0.03}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Test accuracy valutata su reti di lato 30, 50, 80, per rete a singolo layer con 100 neuroni. Ogni valore si riferisce alla media tra i reticoli honeycomb, quadrato e triangolare.}
\label{tab:SigVsReluVsTanhS}
\end{table}
Come si può osservare, per le reti a singolo layer i valori di test accuracy per Sigmoid e Tanh distano al più $1.5\sigma$, mentre la rete implementata con ReLu raggiunge prestazioni nettamente inferiori.

Si è quindi variata la geometria della rete, studiando una rete profonda a 5 layer da 10 neuroni per layer, implementando sia la funzione di attivazione Sigmoid che Relu.
I risultati sono riportati in tabella \ref{tab:SigVsRelu}.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{llll}
\toprule
Rete & L30 TA & L50 TA & L80 TA \\
\midrule
Sigmoid & \num{0.49\pm 0.01} & \num{0.504\pm 0.002} & \num{0.468\pm 0.002}\\
ReLu  & \num{0.964\pm 0.006} & \num{0.982 \pm 0.002} &\num{ 0.877\pm 0.007}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Test accuracy valutata su reti di lato 30, 50, 80, per rete da 5 layer a 10 neuroni per layer. Ogni valore si riferisce alla media tra i reticoli honeycomb, quadrato e triangolare.}
\label{tab:SigVsRelu}
\end{table}
Utilizzando la funzione di attivazione ReLu si osserva un notevole risultato, le sue prestazioni aumentano rispetto alla rete implementata con Sigmoid.
Il risultato giustifica, per questo modello, le scarse prestazioni ottenute attraverso le reti implementate nella sezione \ref{sec:NL}.
La funzione di attivazione ReLu sembra garantire un'elevata  capacità di classificazione per reti a più layer, al contrario dellla funzione di attivazione Sigmoid che pare diminuire notevolmente le performance.

\subsubsection{Ottimizzazione: Adam vs SGD}

\begin{table}[!ht]
\begin{center}
\begin{tabular}{lllll}
\toprule
Rete L30& lr & hc TA & sq TA & tr TA \\
\midrule
Adam &0.0001& \num{0.9886\pm 0.0001} & \num{0.9792\pm 0.0001} & \num{0.9750\pm 0.0002}\\
SGD & 0.001 & \num{0.9875\pm 0.0001} & \num{0.9776 \pm 0.0001} &\num{ 0.9728\pm 0.0002}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Test accuracy valutata su reti implementate sia con ottimizzazione Adam che SGD; per reticolo di lato 30, honeycomb, quadrato e triangolare.}
\label{tab:adaVSsgd}
\end{table}

Tutte le reti analizzate in precedenza sono state implementate attraverso l'ottimizzatore Adam, con un \emph{learning rate} di 0.0001.
Si è provato a variare l'ottimizzatore impostando SGD, ovvero \emph{stochastic gradient descend}, con il medesimo learning rate per una rete a singolo layer con 100 neuroni, implementata con l'attivatore Sigmoid e su reticolo di lato 30.

Tuttavia il sistema per convergere deve effettuare più di 500 epoche, risultando impossibile allenare 10 modelli differenti entro limiti di tempo convenienti.
Quindi si è provato ad alzare il learning rate a 0.001, ottenendo risultati comunque soddisfacenti, riportati in tabella \ref{tab:adaVSsgd}.
Come si può osservare tutti i valori relativi alla rete ottimizzata con Adam distano più di $5\sigma$ rispetto ai valori ottenuti con SGD, risulta quindi sconveniente utilizzare quest'ultimo.

\subsubsection{Diversa batch size}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    title=Honeycomb,
    xlabel = {Batch size},
    ylabel = {Test accuracy},
    xmode=log,
    grid = major,
    width=0.35\textwidth,
    height=0.4\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south west,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=batch, y=accuracy, y error=stdev] {dati/batch_size/batch_hc};
\addlegendentry{L30}
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Quadrato,
    xlabel = {Batch size},
    xmode=log,
    grid = major,
    width=0.35\textwidth,
    height=0.4\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south west,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=batch, y=accuracy, y error=stdev] {dati/batch_size/batch_sq};
\addlegendentry{L30}
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=Triangolare,
    xlabel = {Batch size},
    xmode=log,
    grid = major,
    width=0.35\textwidth,
    height=0.4\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = south west,
]
\addplot+[line, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
    table[x=batch, y=accuracy, y error=stdev] {dati/batch_size/batch_tr};
\addlegendentry{L30}
\end{axis}
\end{tikzpicture}
\caption{Andamento della test accuracy per diversa batch size. I dati provengono da un sistema di Ising di lato 30.}
\label{fig:batch}
\end{figure}

Infine si è provato a variare la dimensione dei batch, ossia il numero di dati utilizzati per un singolo aggiornamento del gradiente.
Come riportato in figura \ref{fig:batch} per batch tra i 10 e i 100 elementi si ottiene una buona test accuracy, aumentandone ulteriormente la dimensione la test accuracy diminuisce.
Dimensioni minori di 10 elementi non sono convenienti da un punto di vista computazionale.
L'analisi è stata effettuata su reticolo di lato 30, allenando una rete a singolo layer con 100 neuroni, funzione di attivazione Sigmoid e ottimizzatore Adam.

\subsection{Apprendimento e magnetizzazione}

\begin{figure}[!ht]
\begin{tikzpicture}
\begin{axis}[
    title=1 neurone - TA \num{0.738 \pm 0.001},
    grid=major,
    xlabel=Magnetizzazione,
    ylabel=Input pesato,
    width=0.35\textwidth,
    height=0.35\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[only marks, mark=star, mark size=1pt]
    table[x=x, y=y] {dati/weights/weights_1neurons};
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=2 neuroni - TA \num{0.9803 \pm 0.0004},
    grid=major,
    xlabel=Magnetizzazione,
    width=0.35\textwidth,
    height=0.35\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[only marks, mark=star, mark size=1pt]
    table[x=x, y=y] {dati/weights/weights_2neurons_1};
\addplot+[only marks, mark=star, mark size=1pt]
    table[x=x, y=y] {dati/weights/weights_2neurons_2};
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title=3 neuroni - TA \num{0.9783 \pm 0.0003},
    grid=major,
    xlabel=Magnetizzazione,
    width=0.35\textwidth,
    height=0.35\textwidth,
    major grid style = {line width=.1pt, dashed, draw=gray!30},
    legend pos = north east,
]
\addplot+[only marks, mark=star, mark size=1pt]
    table[x=x, y=y] {dati/weights/weights_3neurons_1};
\addplot+[only marks, mark=star, mark size=1pt]
    table[x=x, y=y] {dati/weights/weights_3neurons_2};
\addplot+[only marks, mark=star, mark size=1pt]
    table[x=x, y=y] {dati/weights/weights_3neurons_3};
\end{axis}
\end{tikzpicture}
\caption{Valutazione degli input pesati in ingresso all'unico layer nascosto per diverse reti, sono rappresentati solo \num{1000} elementi del test set, in alto viene indicato il numero di neuroni e la test accuracy. L'apprendimento è stato eseguito su sistemi di Ising di lato 30, i modelli sono stati valutati su sistemi a reticolo quadrato. Sull'asse delle ascisse è posta la magnetizzazione media per spin, valutata tramite la formula \ref{eq:magn}. Sull'asse delle ordinate è posto l'input pesato tramite la formula $I_w = Wx + b$, dove $W$ è il vettore dei pesi, $b$ è il bias applicato e $x$ è il vettore di input ossia una configurazione del sistema di spin.}
\label{fig:weights}
\end{figure}

Come è stato mostrato nella sezione \ref{sec:2Ln} esiste una soglia nel numero di neuroni del primo layer al di sotto della quale la rete fa fatica ad apprendere correttamente la fase del sistema con una buona test accuracy.
In quella sezione le reti sono state allenate senza scarti, tenendo conto della media delle performance, mentre in questa sezione le reti sono state allenate fino ad ottenere un unico buon risultato.

Seguendo il procedimento in \cite{carrasqu} è stato rappresentato l'apprendimento dei pesi della rete in figura \ref{fig:weights}, si può notare come la funzione ottenuta dall'applicazione dei pesi sia lineare nella magnetizzazione, che è il parametro d'ordine per la transizione di fase del modello di Ising.
La rete sembra quindi aver appreso il parametro d'ordine di questa transizione di fase.

I grafici in figura \ref{fig:weights} si possono facilmente interpretare, ad esempio nel caso della rete a 2 neuroni si può osservare come uno dei due riceva un input maggiore di zero per magnetizzazione positiva, l'altro per magnetizzazione opposta.
I neuroni hanno Sigmoid come funzione di attivazione (vd. figura \ref{fig:actv_func}).
Nel caso in cui il sistema di spin sia ordinato, quindi con magnetizzazione non nulla, uno dei due neuroni avrà output positivo e l'altro avrà output opposto; nel caso in cui la configurazione degli spin sia disordinata, quindi con magnetizzazione nulla, entrambi i neuroni avranno output negativo.
La rete può riconoscere facilmente in quale delle due fasi si trova il sistema.

\subsection{Stima della temperatura critica}

\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={T [$J/k_B$]},
    ylabel={Output},
    width=0.8\textwidth,
    height=0.4\textwidth,
    grid=both,
    ymin=-0.1,
    ymax=1.1,
    minor grid style={line width=.1pt, dashed, draw=gray!30},
    major grid style={line width=.1pt, dashed, draw=gray!30},
    %legend pos=north east,
    legend style={at={(0.03,0.5)},anchor=west},
]
\addplot[mark=none, black, very thick] coordinates {(2.2692,-0.1) (2.2692,1.1)};
\addlegendentry{T teorica}
\addplot[mark=*, blue, very thick, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
     table [x=temp, y=output1, y error=output1_e] {dati/temperature_cross};
\addlegendentry{Output 1}
\addplot[mark=triangle*, red, very thick, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
     table [x=temp, y=output2, y error=output2_e] {dati/temperature_cross};
\addlegendentry{Output 2}
\end{axis}
\end{tikzpicture}
\caption{Output dei due neuroni presenti nel layer di output, la rete è valutata su configurazioni di un sistema di Ising di lato 30. La temperatura critica per il modello di Ising si trova in tabella \ref{tab:ltI}. I risultati sono ottenuti dalla media su 250 configurazioni per ciascuna temperatura.}
\label{fig:temp_cross}
\end{figure}

Attraverso l'utilizzo delle reti neuronali è stato possibile stimare la temperatura critica $T_c$ del modello di Ising.
Il valore teorico, riportato in tabella \ref{tab:ltI}, è esatto per sistemi su reticolo di lato infinito; per sistemi di lato finito la temperatura critica presenta una correzione che aumenta al diminuire della dimensione del reticolo \cite{fisher}.
Per stimare il valore esatto per dimensione infinità è stato effettuato il calcolo della temperatura critica su diversi reticoli al variare del lato da 20 a 90, con passo 10, andando poi ad interpolare i risultati e ottenendo il valore per $L\rightarrow \infty$ come ordinata all'origine della retta che interpola i dati.
Un esempio di misura della temperatura critica è presentato in figura \ref{fig:temp_cross}, il punto in cui le due curve si incontrano è preso come stima della temperatura critica per quel set di dati.

\begin{table}[ht]
\begin{center}
\begin{tabular}{ll}
\toprule
Layer: & 1\\
Neuroni: & 100 \\
Batch size: & 100\\
Pesi w e b iniziali: & Gauss($\nu=0$, $\sigma=1$)\\
Fz. attivazione: & Sigmoid\\
Earlystop: & validation accuracy \\
Regolarizzazione: & L2 0.01\\
Ottimizzazione: & Adam 0.0001 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Parametri}
\label{tab:ffnnpar}
\end{table}

La rete è stata allenata su set di almeno \num{100000} configurazioni (prendendo un sottoinsieme di configurazioni pari al $20\%$ del training set per la validazione) a reticolo quadrato, e testata su set di \num{100000} per tutti e tre le geometrie di reticolo.
Ciascun valore di temperatura riportato è dato dalla media di 100 temperature, i cui valori sono classificati da 10 diverse reti su 10 diversi test set, come spiegato nella sezione \ref{sec:stats}.
Le reti utilizzate per questo scopo sono state scelte singolarmente tra quelle con test accuracy alta e test loss bassa.
I parametri comuni alle reti sono riportati in tabella \ref{tab:ffnnpar}.

\begin{figure}[ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={Honeycomb},
    ylabel={T [$J/k_B$]},
    xlabel={$L^{-1}$  [$\si{nspin}^{-1}$]},
    width=0.33\textwidth,
    height=0.44\textwidth,
    xmin=0.0,
    xmax=0.06,
    minor y tick num=1,
    ytick={1.51,1.52,1.53,1.54},
    grid=both,
    minor grid style={line width=.1pt, dashed, draw=gray!30},
    major grid style={line width=.1pt, dashed, draw=gray!30},
    legend pos=north west
]
\addplot+[mark=none, red, thick] coordinates {(0.0,1.5187) (0.06,1.5187)};
\addlegendentry{$T_c$}
\addplot[only marks, blue, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
     table [x expr={1/\thisrow{size}}, y=temp, y error=temp_e] {dati/temperature_hc};
\addlegendentry{$T_L$}
\addplot+[mark=none, green!70!black, thick, domain=0.01:0.055, samples=101]{0.2939*x + 1.5151};
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title={Quadrato},
    xlabel = {$L^{-1}$  [$\si{nspin}^{-1}$]},
    width=0.33\textwidth,
    height=0.44\textwidth,
    xmin=0.0,
    xmax=0.06,
    minor y tick num=1,
    ytick={2.25,2.26,2.27,2.28,2.29,2.30},
    grid=both,
    minor grid style={line width=.1pt, dashed, draw=gray!30},
    major grid style={line width=.1pt, dashed, draw=gray!30},
    legend pos=north west
]
\addplot+[mark=none, red, thick] coordinates {(0.0,2.2692) (0.06,2.2692)};
\addlegendentry{$T_c$}
\addplot[only marks, blue, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
     table [x expr={1/\thisrow{size}}, y=temp, y error=temp_e] {dati/temperature_sq};
\addlegendentry{$T_L$}
\addplot+[mark=none, green!70!black, thick, domain=0.01:0.055, samples=101]{0.4167*x + 2.2653};
\end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}
\begin{axis}[
    title={Triangolare},
    xlabel = {$L^{-1}$  [$\si{nspin}^{-1}$]},
    width=0.33\textwidth,
    height=0.44\textwidth,
    xmin=0.0,
    xmax=0.06,
    minor y tick num=1,
    ytick={3.62,3.63,3.64,3.65,3.66},
    grid=both,
    minor grid style={line width=.1pt, dashed, draw=gray!30},
    major grid style={line width=.1pt, dashed, draw=gray!30},
    legend pos=north west
]
\addplot+[mark=none, red, thick] coordinates {(0.0,3.6410) (0.06,3.6410)};
\addlegendentry{$T_c$}
\addplot[only marks, blue, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
     table [x expr={1/\thisrow{size}}, y=temp, y error=temp_e] {dati/temperature_tr};
\addlegendentry{$T_L$}
\addplot+[mark=none, green!70!black, thick, domain=0.01:0.055, samples=101]{0.3994*x + 3.6348};
\end{axis}
\end{tikzpicture}
\caption{Interpolazione temperatura}
\label{fig:Tlimit}
\end{figure}

\begin{table}[!ht]
\begin{center}
\begin{tabular}{lll}
\toprule
Reticolo & T estratta [$J/k_B$] & T teorica [$J/k_B$] \\
\midrule
honeycomb & \num{1.52 \pm 0.04} & \num{1.519} \\
quadrato & \num{2.27 \pm 0.07} & \num{2.269} \\
triangolare & \num{3.63 \pm 0.16} & \num{3.641} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Valori ottenuti tramite il fit per la temperatura di transizione. I valori teorici sono riportati con maggiore precisione in tabella \ref{tab:ltI}.}
\label{tab:Tising}
\end{table}

Nei grafici in figura \ref{fig:Tlimit} sono riportate le medie delle temperature $T_L$ al variare del lato del reticolo, con le relative barre d'errore date dalla deviazione standard della media.
Inoltre è riportato il valore teorico della temperatura di transizione $T_c$ e la retta che interpola tali temperature, il fit è stato eseguito con il metodo dei minimi quadrati.
In tabella \ref{tab:Tising} sono riportati i valori estratti per la temperatura di transizione.

%----------------------------------------------------------------------------------------
%	LAST SECTION
%----------------------------------------------------------------------------------------

\section{Modello XY: risultati e conclusioni}
Contrariamente al modello di Ising, per sistemi XY non è noto un parametro d'ordine che manifesti la transizione di fase, quindi non si può stabilire con certezza cosa apprenda la rete.
La letteratura \cite{melko} suggerisce che per sistemi finito dimensionali, come quelli studiati in questa relazione, il sistema sia parzialmente magnetizzato e quindi si suppone che la rete allenata sulle configurazioni di spin possa apprendere questa leggera magnetizzazione dovuta alla non idealità del sistema.
Perciò, come riportato nell'introduzione, si è provato ad allenare la rete anche sulle configurazioni di vortici ottenendo risultati simili a quelli relativi alle configurazioni.
Tuttavia, sempre come osservato nell'articolo \cite{melko}, non è chiaro cosa la rete riesca ad apprendere e sopratutto se l'apprendimento sia reso possibile unicamente dalla non idealità del sistema.

In questo lavoro viene effettuata solo una breve analisi sul modello XY, giustificata principalmente dall'interesse fisico, approfondimenti significativi sulla parte di apprendimento richiederebbero risorse computazionali di gran lunga maggiori.
La struttura di rete da cui si è partiti corrisponde a quella rappresentata in figura \ref{fig:cnn}.

\subsection{Breve valutazione dei parametri della rete}
Analogamente a quanto svolto per il modello di Ising si è provato a variare alcuni parametri significativi della rete convoluzionale utilizzata per il modello XY, testandola sia su configurazioni di dati non elaborati (quindi su valori in $[0, 2\pi)$) che sulle configurazioni di vortici (quindi su valori in $\{1;0;-1\}$), sempre per un reticolo a quattro primi vicini di lato 32.
Innanzi tutto si è provato a variare il kernel del primo layer convoluzionale, allenando e testando la rete per un kernel 2$\times$2 e 3$\times$3.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lll}
\toprule
Kernel 1\textsuperscript{o} layer & Configurazioni & Vortici \\
\midrule
2$\times$2 &\num{0.9738 \pm 0.0002} & \num{0.9743 \pm 0.0002}\\
3$\times$3 & \num{0.9729 \pm 0.0002} &\num{0.9743 \pm 0.0002}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Confronto tra reti con diversa dimensione del kernel nel primo layer convoluzionale, per il resto la struttura delle reti è identica a quella in figura \ref{fig:cnn}.}
\label{tab:2x2vs3x3}
\end{table}

Come si può osservare dai risultati riportati in tabella \ref{tab:2x2vs3x3}, la test accuracy per le configurazioni classificate con kernel 2$\times$2 risulta essere superiore di $3\sigma$ rispetto alle configurazioni classificate con la rete avente kernel 3$\times$3. Al contrario per i vortici si ottiene esattamente lo stesso valore.
La differenza di performance nel caso delle semplici configurazioni di spin si può imputare alla struttura delle caratteristiche che la rete deve apprendere, i vortici e gli antivortici possono infatti essere identificati su sottomatrici di lato 2, come mostrato in figura \ref{fig:xy}.

Successivamente si è variata la geometria del layer nascosto a 128 neuroni, allenando una rete a due layer da 16 neuroni ciascuno.
L'analisi è stata effettuata sia sul set di configurazioni che sul set di vortici, per una rete con un kernel 2$\times$2 per il primo layer convoluzionale.
In tabella \ref{tab:16x16} si confrontano i risultati dell'analisi.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lll}
\toprule
NN Layer Dense & Configurazioni & Vortici \\
\midrule
128 &\num{0.9738 \pm 0.0002} & \num{0.9743 \pm 0.0002}\\
16+16 & \num{0.9722 \pm 0.0003} &\num{0.9740 \pm 0.0002}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Confronto fra le test accuracy di una rete convoluzionale con un singolo layer nascosto da 128 neuroni e una con due layer da 16 neuroni ciascuno.}
\label{tab:16x16}
\end{table}

In entrambi i casi risultano superiori le prestazioni per la rete con singolo layer da 128 neuroni, sebbene nel caso dei vortici la test accuracy sia superiore di appena $1\sigma$.

\subsection{CNN vs FFNN}
Per verificare gli effettivi vantaggi di una rete convoluzionale nel caso in cui non si forniscano alla rete le caratteristiche da apprendere, si è provato a confrontare le prestazioni in classificazione fra una rete convoluzionale e una rete feed forward a singolo layer.
L'analisi è stata effettuata sempre per il reticolo di lato 32, sia su configurazioni di vortici che di spin.
La rete utilizzata è a singolo layer da 128 neuroni, con attivatore Sigmoid, ottimizzatore Adam e i vari metodi descritti in precedenza per evitare l'overfitting.

\begin{table}[ht]
\begin{center}
\begin{tabular}{lll}
\toprule
Rete & Configurazioni & Vortici \\
\midrule
FFNN &\num{0.9116 \pm 0.0004} & \num{0.9610 \pm 0.0002}\\
CNN & \num{0.9738 \pm 0.0002} & \num{0.9743 \pm 0.0002}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Confronto tra una rete feed forward a singolo layer nascosto da 128 neuroni e la rete convoluzionale più performante (vd. tabella \ref{tab:2x2vs3x3}).}
\label{tab:FFNNvsCNN}
\end{table}

Come si può osservare dai risultati riportati in tabella \ref{tab:FFNNvsCNN}, la rete convoluzionale risulta essere nettamente più performante, giustificando le scelte effettuate in precedenza.
Infatti una rete priva dei layer convoluzionali fatica ad apprendere le caratteristiche del sistema, in particolare nel caso in cui vengono fornite in input le sole configurazioni degli spin.

\subsection{Stima della temperatura di transizione}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={Configurazioni},
    ylabel={T [$J/k_B$]},
    xlabel = {$ln(L)^{-2}$  [$\si{ln(nspin)}^{-2}$]},
    width=0.4\textwidth,
    height=0.44\textwidth,
    xmin=0.0,
    xmax=0.15,
    minor y tick num=1,
    ytick={0.87,0.88,0.89, 0.9},
    grid=both,
    minor grid style={line width=.1pt, dashed, draw=gray!30},
    major grid style={line width=.1pt, dashed, draw=gray!30},
    legend pos=north east
]
\addplot+[mark=none, red, thick] coordinates {(0.0,0.893) (0.15,0.893)};
\addlegendentry{$T_t$}
\addplot[only marks, blue, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
     table [x expr={1/ln(\thisrow{size})^2}, y=temp, y error=temp_e] {dati/temperature_cg};
\addlegendentry{$T_L$}
\addplot+[mark=none, green!70!black, thick, domain=0.06:0.13, samples=101]{-0.2297*x + 0.9081};
\end{axis}
\end{tikzpicture}
\hspace{2cm}
\begin{tikzpicture}
\begin{axis}[
    title={Vortici},
    ylabel={T [$J/k_B$]},
    xlabel = {$ln(L)^{-2}$  [$\si{ln(nspin)}^{-2}$]},
    width=0.38\textwidth,
    height=0.44\textwidth,
    xmin=0.0,
    xmax=0.15,
    minor y tick num=1,
    ytick={0.87,0.88,0.89, 0.9},
    grid=both,
    minor grid style={line width=.1pt, dashed, draw=gray!30},
    major grid style={line width=.1pt, dashed, draw=gray!30},
    legend pos=north east
]
\addplot+[mark=none, red, thick] coordinates {(0.0,0.893) (0.15,0.893)};
\addlegendentry{$T_t$}
\addplot[only marks, blue, mark=*, error bars/.cd, y dir = both, y explicit, error mark=none, error bar style=thick]
     table [x expr={1/ln(\thisrow{size})^2}, y=temp, y error=temp_e] {dati/temperature_vx};
\addlegendentry{$T_L$}
\addplot+[mark=none, green!70!black, thick, domain=0.06:0.13, samples=101]{-0.2297*x + 0.9081};
\end{axis}
\end{tikzpicture}
\caption{Interpolazione temperatura}
\label{fig:xyTlimit}
\end{figure}

\begin{table}[!ht]
\begin{center}
\begin{tabular}{lll}
\toprule
Dati & T estratta [$J/k_B$] & T teorica [$J/k_B$] \\
\midrule
configurazioni & \num{ 0.0 \pm 0.0} & \num{0.893}\\
vortici & \num{0.91 \pm0.05}  & \num{0.893} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Valori ottenuti tramite il fit per la temperatura di transizione.}
\label{tab:Txy}
\end{table}

Anche per il modello XY si è provato a stimare la temperatura a cui avviene la transizione di fase, sia utilizzando le semplici configurazioni sia utilizzando i vortici.
Basandosi sui risultati riportati in tabella \ref{tab:2x2vs3x3}, sono state allenate due reti convoluzionali differenti per le due tipologie di dati in input.
Per la classificazione basata sulle configurazioni di spin è stata allenata una rete avete il primo layer convoluzionale con kernel 2$\times$2, il secondo 3$\times$3 e un layer fully connected a 128 neuroni.
Invece per la rete che classifica basandosi sui vortici è stato impostato il primo layer convoluzionale con kernel 3$\times$3, mantenendo le altre caratteristiche invariate.

Nei grafici di figura \ref{fig:xyTlimit} sono riportati le temperature e gli errori relativi ai reticoli di lato 16, 24, 32, 40 e 48; in tabella \ref{tab:Txy} sono indicati i valori estratti tramite i due procedimenti.

Entrabi i grafici presentano un andamento anomalo rispetto a quanto ci si aspetterebbe infatti, come riportato in \cite{melko}, la temperatura di transizione teorica per reticoli di lato finito $T_L=T(L)$ è legata alla temperatura di transizione del sistema ideale nel limite termodinamico $T_t$ dalla formula
\begin{equation}
T(L) \approx T_t + \frac{\pi^2}{4c \ln{L}^2}
\label{formula:TL}
\end{equation}
quindi ci si aspetterebbe una convergenza a $T_t$ dall'alto.

Come si può osservare dal primo grafico riportato in figura \ref{fig:xyTlimit}, ovvero quello relativo alle configurazioni non elaborate in vortici, i valori per le diverse dimensioni di reticolo presentano barre d'errore estese.
La retta che interpola i dati (da concludere, in attesa dell'ultimo dato!) \dots

D'altra parte le temperature estrapolate dalla classificazione di configurazioni di vortici presentano barre d'errore contenute, e vengono interpolate adeguatamente dalla retta riportata nel grafico.
Inoltre, come si può vedere in tabella \ref{tab:Txy}, nonostante l'andamento differente dalla predizione teorica \ref{formula:TL} si ottiene una buona stima della temperatura di transizione, largamente compatibile entro l'errore con la temperatura teorica.

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{ieeetr}

\bibliography{relazione}

%----------------------------------------------------------------------------------------


\end{document}
